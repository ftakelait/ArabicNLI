{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages\n!pip install transformers datasets torch seaborn evaluate scikit-learn sentencepiece accelerate -U kornia","metadata":{"id":"38361ca8","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://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@github.com/ftakelait/ArabicNLI.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary modules\n\n# For warning control\nimport warnings\n\n# For manipulating data\nimport os\nimport pandas as pd\n\n# For numerical operations\nimport numpy as np\n\n# For plotting and visualizing data\nimport matplotlib.pyplot as plt\n\n# For advanced training functionality\nfrom accelerate import Accelerator\n\n# For deep learning and data processing\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom transformers import AutoModel, BertTokenizerFast, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nfrom tqdm import tqdm_notebook\nfrom datasets import load_dataset, load_metric, Dataset\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\n\n# For logging and tracking experiment metrics\nimport wandb\n\n# For additional data processing and model training\nimport re\nimport seaborn as sns\nimport sentencepiece as spm\n\nimport sys\nsys.path.append('/kaggle/working/ArabicNLI/')\n\nfrom src.loss_function import *  # assuming you have a loss_function file in the src directory\n\nwarnings.filterwarnings('ignore')","metadata":{"id":"d5f9ef9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Login to wandb\n!wandb login fd4d1a0bfccb7888acd4ea98e25eeba73b66a92e\n\n# Initialize wandb run\nwandb.init(project=\"ArabicNLI\", entity='ftakelait')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of models to be used\nlist_of_models  =[\"UBC-NLP/MARBERT\", \"qarib/bert-base-qarib\", \"aubmindlab/bert-base-arabertv02-twitter\", 'aubmindlab/bert-large-arabertv02']\n\n# Select the first model from the list\nmodel_name=list_of_models[0]\n\n# Extract model_checkpoint from model_name\nmodel_checkpoint = model_name.split(\"/\")[-1]\n\n# Training parameters\nbatch_size = 16\nlearning_rate = 0.00002\nepochs = 4","metadata":{"id":"686381ba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load metrics\nacc_metric = load_metric('accuracy')\nf1_metric = load_metric('f1')\nprecision_metric = load_metric('precision')\nrecall_metric = load_metric('recall')","metadata":{"id":"2269c509","outputId":"9db5129c-224a-4f7b-86cc-07311c1d156e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred: tuple) -> dict:\n    \"\"\"\n    Compute evaluation metrics: accuracy, F1 score, precision, and recall.\n\n    Args:\n        eval_pred (tuple): Tuple containing predictions and labels.\n\n    Returns:\n        dict: Dictionary containing accuracy, F1 score, precision, and recall.\n    \"\"\"\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n\n    accuracy = acc_metric.compute(predictions=predictions, references=labels)\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n    precision = precision_metric.compute(predictions=predictions, references=labels, average='macro')\n    recall = recall_metric.compute(predictions=predictions, references=labels, average='macro')\n\n    return {\"accuracy\": accuracy['accuracy'], \"f1\": f1['f1'], \"precision\": precision['precision'], \"recall\": recall['recall']}","metadata":{"id":"d23c8a4a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upload_dataset(train_file: str, valid_file: str, test_file: str) -> Dataset:\n    \"\"\"\n    Load datasets from provided CSV files.\n\n    Args:\n        train_file (str): Path to the training data file.\n        valid_file (str): Path to the validation data file.\n        test_file (str): Path to the testing data file.\n\n    Returns:\n        Dataset: HuggingFace Dataset object containing train, validation, and test data.\n    \"\"\"\n    dataset = load_dataset(\"csv\", data_files=train_file)\n    val_data = pd.read_csv(valid_file)\n    ds_val = Dataset.from_pandas(val_data)\n\n    test_data = pd.read_csv(test_file)\n    ds_test = Dataset.from_pandas(test_data)\n\n    dataset[\"validation\"] = ds_val\n    dataset[\"test\"] = ds_test\n\n    return dataset","metadata":{"id":"26663f5e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the max length of the input sequence for each model\nMAX_LEN = 90      # MARBERT ArbTEDS\n\n# MAX_LEN = 96    #MARBERT\n# MAX_LEN = 60    #qarib/bert-base-qarib\n# MAX_LEN = 52    #aubmindlab/bert-base-arabertv02-twitter / aubmindlab/bert-large-arabertv02\n\n# MAX_LEN = 329   #MARBERT XLNI\n# MAX_LEN = 359   #qarib/bert-base-qarib\n# MAX_LEN = 329   #aubmindlab/bert-base-arabertv02-twitter / aubmindlab/bert-large-arabertv02 X_NLI\n\n# MAX_LEN = 97    #qarib/bert-base-qarib\n# MAX_LEN = 88    #aubmindlab/bert-base-arabertv02-twitter / aubmindlab/bert-large-arabertv02 X_NLI\n\ndef preprocess_function(examples: dict) -> dict:\n    \"\"\"\n    Preprocess function for tokenization.\n\n    Args:\n        examples (dict): Dictionary containing examples to be tokenized.\n\n    Returns:\n        dict: Dictionary containing tokenized examples.\n    \"\"\"\n    return tokenizer(examples['t'], examples['h'], truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n\n\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n\n# Initialize the model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n# Load the ArbTEDS dataset\ndataset = upload_dataset(\n    '/kaggle/working/ArabicNLI/dataset/ArbTEDS/train_ArbTEDS.csv', \n    '/kaggle/working/ArabicNLI/dataset/ArbTEDS/valid_ArbTEDS.csv', \n    '/kaggle/working/ArabicNLI/dataset/ArbTEDS/test_ArbTEDS.csv')\n\n# Preprocess the ArbTEDS dataset\nencoded_dataset = dataset.map(preprocess_function)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set training arguments\nargs = TrainingArguments(\n    f\"{model_checkpoint}_checkpoints\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=learning_rate,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=epochs,\n    load_best_model_at_end=True,\n)\n\n# Initialize the trainer\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset['validation'],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()","metadata":{"id":"1324014d","outputId":"244c95c2-3580-4a67-d26d-3977b0239947","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test dataset\ndef predict_test_dataset(trainer: Trainer, dataset: Dataset) -> 'np.ndarray':\n    \"\"\"\n    Make predictions on the test dataset.\n\n    Args:\n        trainer (Trainer): The trainer instance.\n        dataset (Dataset): The preprocessed test dataset.\n\n    Returns:\n        np.ndarray: Predictions.\n    \"\"\"\n    predictions = trainer.predict(dataset[\"test\"])\n    return predictions\n\n# Get and print predictions\npredictions = predict_test_dataset(trainer, encoded_dataset)\nprint(predictions)","metadata":{"id":"b61722de","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}